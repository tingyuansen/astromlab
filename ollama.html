<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Running Models on Your Laptop &middot; AstroMLab
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="./public/css/poole.css">
  <link rel="stylesheet" href="./public/css/syntax.css">
  <link rel="stylesheet" href="./public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="./atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          AstroMLab
        </a>
      </h1>
      <p class="lead">Steering astronomy into the age of autonomy! <br> Builder of AstroLLaMA and AstroSage</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="./index.html">Home</a>

      

      
      
        
      
        
          
            <a class="sidebar-nav-item" href="./benchmarking.html">AstroBench</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item active" href="./ollama.html">Running Models on Your Laptop</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="./why_astronomy.html">Why LLMs for Astronomy?</a>
          
        
      

      <a class="sidebar-nav-item" href="https://huggingface.co/AstroMLab">Download Models on Hugging Face</a> 
    </nav>

    <p>&copy; 2024. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="page">
        <h1 class="page-title">Running Models on Your Laptop</h1>
      <div class="page">
  <p>We have released <strong>AstroSage-LLaMA-3.1-8B</strong> (<a href="https://arxiv.org/abs/2411.09012">de Haan et al. 2024</a>), our flagship 8B parameter model for astronomy, now available in GGUF format for efficient local deployment.</p>

<p>The model is available on <a href="https://huggingface.co/AstroMLab/AstroSage-8B-GGUF">Hugging Face</a> in two quantized versions:</p>

<ul>
  <li><strong>AstroSage-8B-BF16.gguf</strong>: BFloat16 precision for maximum accuracy</li>
  <li><strong>AstroSage-8B-Q8_0.gguf</strong>: 8-bit quantized for more efficient deployment</li>
</ul>

<p>Key Features:</p>
<ul>
  <li>CPU-friendly inference on standard laptops</li>
  <li>Compatible with llama.cpp ecosystem</li>
  <li>Maintains core capabilities of the original model while optimizing for local deployment</li>
</ul>

<p>For detailed installation instructions, usage examples, and comprehensive documentation, please visit our <a href="https://huggingface.co/AstroMLab/AstroSage-8B-GGUF">Hugging Face model page</a>.</p>

</div>

      </div>
    </div>

  </body>
</html>
