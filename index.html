<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Welcome to AstroMLab &middot; AstroMLab
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="./public/css/poole.css">
  <link rel="stylesheet" href="./public/css/syntax.css">
  <link rel="stylesheet" href="./public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="./atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          AstroMLab
        </a>
      </h1>
      <p class="lead">Steering astronomy into the age of autonomy! <br> Builder of AstroLLaMA</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item active" href="./index.html">Home</a>

      

      
      
        
      
        
          
            <a class="sidebar-nav-item" href="./benchmarking.html">AstroBench</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="./ollama.html">Running Models on Your Laptop</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="./why_astronomy.html">Why LLMs for Astronomy?</a>
          
        
      

      <a class="sidebar-nav-item" href="https://huggingface.co/AstroMLab">Download Models on Hugging Face</a> 
    </nav>

    <p>&copy; 2024. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="page">
        <h1 class="page-title">Welcome to AstroMLab</h1>
      <h2 id="who-we-are">Who We Are</h2>

<p>AstroMLab is a dynamic group of <em>astronomers</em> and <em>computer scientists</em> passionate about pushing the boundaries of <strong>Large Language Models (LLMs)in astronomy</strong>. Our team includes:</p>

<ul>
  <li><em>Leading astronomers</em></li>
  <li><em>Top natural language processing experts</em> from Oak Ridge National Laboratory and Argonne National Laboratory</li>
  <li><em>Frontier arXivists</em> from the NASA Astrophysics Data System</li>
  <li><em>Enthusiastic young researchers</em> bridging the gap between astronomy and LLMs</li>
</ul>

<p>While LLMs are advancing rapidly, we believe that real progress in <em>AI-driven astronomical research</em> requires <em>deep domain knowledge</em>. This conviction drives us to tackle the challenges in applying LLMs to astronomy head-on.</p>

<h2 id="our-goals">Our Goals</h2>

<p>Our ultimate aim is to:</p>

<ol>
  <li>Develop specialized LLMs for astronomy</li>
  <li>Create <strong>reliable, light-weight, and open-source models</strong> adaptable for advanced research agents</li>
  <li><strong>Expedite scientific discovery</strong> through LLM-driven end-to-end research</li>
  <li>Push the boundaries of what’s possible in astronomical research</li>
</ol>

<h2 id="our-achievements">Our Achievements</h2>

<p>Despite being a young group, we’ve made significant strides:</p>

<ul>
  <li>Curated the first extensive astronomy-based benchmarking dataset using high-quality review articles</li>
  <li>Explored training of specialized astronomy LLMs</li>
  <li>Released three model sets:
    <ul>
      <li>AstroLLaMA-2-70B</li>
      <li>AstroLLaMA-3-8B</li>
      <li>AstroLLaMA-2-7B (developed during our time at <em>UniverseTBD</em>)</li>
    </ul>
  </li>
</ul>

<h2 id="open-source-commitment">Open Source Commitment</h2>

<p>We are fully committed to open source:</p>

<ul>
  <li>All our models are released on <strong>Hugging Face</strong></li>
  <li>Find our models here: <a href="https://huggingface.co/AstroMLab">AstroMLab on Hugging Face</a></li>
</ul>

<h2 id="our-support-and-vision">Our Support and Vision</h2>

<p>We are grateful for our supporters:</p>

<ul>
  <li>Access to the Frontier nodes at Oak Ridge Leadership Computing Facility</li>
  <li>Backing from Microsoft’s Accelerating Foundation Models Research (AFMR)</li>
</ul>

<h2 id="join-us">Join Us</h2>

<p>Our team is expanding, and we’d love to hear from you!</p>

<ul>
  <li>Contact us: <a href="mailto:ting.74@osu.edu">ting.74@osu.edu</a></li>
</ul>

<p><br /></p>

<hr />

<h2 id="team">Team</h2>

<table>
  <tr>
    <td align="center" width="25%"><img src="figures/Members_Yuan-Sen_Ting.png" alt="Yuan-Sen Ting" /></td>
    <td align="center" width="25%"><img src="figures/Members_Tirthankar_Ghosal.png" alt="Tirthankar Ghosal" /></td>
    <td align="center" width="25%"><img src="figures/Members_Tijmen_de_Haan.png" alt="Tijmen de Haan" /></td>
    <td align="center" width="25%"><img src="figures/Members_Josh_Nguyen.png" alt="Josh Nguyen" /></td>
  </tr>
  <tr>
    <td align="center"><strong>Yuan-Sen Ting</strong><br />The Ohio State University</td>
    <td align="center"><strong>Tirthankar Ghosal</strong><br />Oak Ridge National Laboratory</td>
    <td align="center"><strong>Tijmen de Haan</strong><br />KEK, High Energy Accelerator Research Organization</td>
    <td align="center"><strong>Josh Nguyen</strong><br />University of Pennsylvania</td>
  </tr>
  <tr>
    <td align="center"><img src="figures/Members_Rui_Pan.png" alt="Rui Pan" /></td>
    <td align="center"><img src="figures/Members_Hardik_Arora.png" alt="Hardik Arora" /></td>
    <td align="center"><img src="figures/Members_Emily_Herron.png" alt="Emily Herron" /></td>
    <td align="center"><img src="figures/Members_Yuwei_Yang.png" alt="Yuwei Yang" /></td>
  </tr>
  <tr>
    <td align="center"><strong>Rui Pan</strong><br />University of Illinois Urbana-Champaign</td>
    <td align="center"><strong>Hardik Arora</strong><br />Indian Institutes of Technology</td>
    <td align="center"><strong>Emily Herron</strong><br />Oak Ridge National Laboratory</td>
    <td align="center"><strong>Yuwei Yang</strong><br />Australian National University</td>
  </tr>
  <tr>
    <td align="center"><img src="figures/Members_Zechang_Sun.png" alt="Alberto Accomazzi" /></td>
    <td align="center"><img src="figures/Members_Alberto_Accomazzi.png" alt="Alberto Accomazzi" /></td>
    <td align="center"><img src="figures/Members_Argonne.png" alt="Azton Wells" /></td>
    <td align="center"><img src="figures/Members_Nesar_Ramachandra.png" alt="Nesar Ramachandra" /></td>
    <td align="center"><img src="figures/Members_Sandeep_Madireddy.png" alt="Sandeep Madireddy" /></td>
  </tr>
  <tr>
    <td align="center"><strong>Zechang Sun</strong><br />Tsinghua University</td>
    <td align="center"><strong>Alberto Accomazzi</strong><br />NASA Astrophysics Data System</td>
    <td align="center"><strong>Azton Wells</strong><br />Argonne National Laboratory</td>
    <td align="center"><strong>Nesar Ramachandra</strong><br />Argonne National Laboratory</td>
  </tr>
  <tr>
    <td align="center"><img src="figures/Members_Sandeep_Madireddy.png" alt="Sandeep Madireddy" /></td>
  </tr>
  <tr>
    <td align="center"><strong>Sandeep Madireddy</strong><br />Argonne National Laboratory</td>
  </tr>
</table>

<p><br /></p>

<hr />

<h2 id="key-outputs">Key Outputs</h2>

<h3 id="astromlab-1-who-wins-astronomy-jeopardy">AstroMLab 1: Who Wins Astronomy Jeopardy!?</h3>

<p><strong><a href="https://arxiv.org/abs/2407.11194">Yuan-Sen Ting, et al., 2024, arXiv:2407.11194</a></strong></p>

<p>We present a comprehensive evaluation of proprietary and open-weights large language models using the first astronomy-specific benchmarking dataset. This dataset comprises 4,425 multiple-choice questions curated from the Annual Review of Astronomy and Astrophysics, covering a broad range of astrophysical topics.</p>

<p>Key findings:</p>
<ul>
  <li>Claude-3.5-Sonnet outperforms competitors, achieving 85.0% accuracy.</li>
  <li>Open-weights models like LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now compete with some of the best proprietary models.</li>
  <li>We identify performance variations across astronomical subfields, with challenges in exoplanet-related fields, stellar astrophysics, and instrumentation.</li>
  <li>Top-performing models demonstrate well-calibrated confidence, with correlations above 0.9 between confidence and correctness.</li>
  <li>The rapid progress suggests that LLM-driven research in astronomy may become feasible in the near future.</li>
</ul>

<p><br /></p>

<h3 id="astromlab-2-astrollama-2-70b-model-and-benchmarking-specialised-llms-for-astronomy">AstroMLab 2: AstroLLaMA-2-70B Model and Benchmarking Specialised LLMs for Astronomy</h3>

<p><strong><a href="https://arxiv.org/abs/2407.11194">Rui Pan, Josh Nguyen, et al., 2024</a></strong></p>

<p>We introduce new models: AstroLLaMA-3-8B and AstroLLaMA-2-70B, building upon the previous AstroLLaMA series and quantitatively assess specialized LLMs in astronomy, leveraging recently curated high-quality astronomical MCQs.</p>

<p>Key points:</p>
<ul>
  <li>Previously released AstroLLaMA series (based on LLaMA-2-7B) underperforms compared to the native LLaMA model.</li>
  <li>Performance degradation can be partially mitigated by using high-quality data for continual pretraining.</li>
  <li>Continual pretraining on the 70B model can yield improvements, despite observed catastrophic forgetting in smaller models.</li>
</ul>

<p><br /></p>

<h3 id="legacy-output-the-astrollama-series">Legacy Output: The AstroLLaMA Series</h3>

<ol>
  <li><strong><a href="https://arxiv.org/abs/2309.06126">Josh Nguyen, et al., 2023, arXiv:2309.06126</a></strong></li>
  <li><strong><a href="https://arxiv.org/abs/2401.01916">Ernest Perkowski, Rui Pan, et al., 2024, arXiv:2401.01916</a></strong></li>
</ol>

<p>The first open-source conversational AI tool tailored for the astronomy community – AstroLLaMA-2-7B and AstroLLaMA-2-7B-Chat.</p>


      </div>
    </div>

  </body>
</html>
